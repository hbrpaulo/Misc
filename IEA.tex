\section{Lehmann Casella - Theory of point estimation}

\subsection{Inadmissibilidade}
Um estimador \( \delta \) é dito ser \textit{inadmissível} se existe outro estimador \( \delta' \) que o domina (isto é, tal que \( R(\theta, \delta') \leq R(\theta, \delta) \) para todo \( \theta \), com estrita desigualdade para algum \( \theta \)) e \textit{admissível} se nenhum tal estimador \( \delta' \) existe.

\subsection{Definição 1.1 - Estimador minimax} Um estimador $\delta^M$ de $\theta$, que minimiza o risco máximo, isto é, que satisfaz
\[
\inf_{\delta} \sup_{\theta} R(\theta, \delta) = \sup_{\theta} R(\theta, \delta^M),
\]
é chamado de \textit{estimador minimax}.

\subsection{Definição 1.3 - Distribuição \texorpdfstring{\(\Lambda\)}{undefined}} Uma distribuição a priori $\Lambda$ é \textit{menos favorável} se $r_{\Lambda} \geq r_{\Lambda'}$ para todas as distribuições a priori $\Lambda'$.

Esta é a distribuição a priori que causa ao estatístico a maior perda média.

O seguinte teorema fornece uma condição simples para um estimador de Bayes $\delta_{\Lambda}$ ser minimax.

\subsection{Teorema 1.4 - Distribuição \texorpdfstring{\(\Lambda\)}{undefined}}
Suponha que $\Lambda$ é uma distribuição em $\Theta$ tal que
\[
r(\Lambda, \delta_{\Lambda}) = \int R(\theta, \delta_{\Lambda}) d\Lambda(\theta) = \sup_{\theta} R(\theta, \delta_{\Lambda}).
\]
Então:
\begin{itemize}
\item[(i)] $\delta_{\Lambda}$ é minimax.
\item[(ii)] Se $\delta_{\Lambda}$ é a única solução de Bayes com respeito a $\Lambda$, ela é o único procedimento minimax.
\item[(iii)] $\Lambda$ é menos favorável.
\end{itemize}

\subsection{Corolário 1.5 - Risco constante}
Se uma solução de Bayes $\delta_{\Lambda}$ tem risco constante, então ela é minimax.

\subsection{Corolário 1.6 - Conjunto \texorpdfstring{\(\omega_\Lambda\)}{undefined}} 
Seja $\omega_{\Lambda}$ o conjunto de pontos de parâmetros nos quais a função de risco de $\delta_{\Lambda}$ atinge seu máximo, isto é,
\[
\omega_{\Lambda} = \{\theta : R(\theta, \delta_{\Lambda}) = \sup_{\theta'} R(\theta', \delta_{\Lambda})\}.
\]
Então, $\delta_{\Lambda}$ é minimax se e somente se
\[
\Lambda(\omega_{\Lambda}) = 1.
\]
Isso pode ser reexpresso dizendo que uma condição suficiente para $\delta_{\Lambda}$ ser minimax é que existe um conjunto $\omega$ tal que
\[
\Lambda(\omega) = 1
\]
e

R\((\theta, \delta_{\Lambda})\) atinge seu máximo em todos os pontos de $\omega$.

\subsection{Lema 1.10 - Linearidade do Estimador de Bayes}
Seja $\delta$ um estimador de Bayes (respectivamente, UMVU, minimax, admissível) de $g(\theta)$ para perda de erro quadrado. Então, $a\delta + b$ é Bayes (respectivamente, UMVU, minimax, admissível) para $ag(\theta) + b$.

\subsection{Definição 1.11 - Risco de Bayes sob \texorpdfstring{\(\Lambda_n\)}{undefined} } 
Uma sequência de distribuições a priori $\{\Lambda_n\}$ é \textit{menos favorável} se para cada distribuição a priori $\Lambda$ temos
\[
r_{\Lambda} \leq r = \lim_{n \to \infty} r_{\Lambda_n},
\]
onde
\[
r_{\Lambda_n} = \int R(\theta, \delta_n) d\Lambda_n(\theta)
\]
é o risco de Bayes sob $\Lambda_n$.

\subsection{Teorema 1.12 - Sequência \texorpdfstring{\(\Lambda_n\)}{undefined} menos favorável} Suponha que $\{\Lambda_n\}$ é uma sequência de distribuições a priori com riscos de Bayes $r_n$ satisfazendo
\[
r_{\Lambda} \leq r = \lim_{n \to \infty} r_{\Lambda_n}
\]
e que $\delta$ é um estimador para o qual
\[
\sup_{\theta} R(\theta, \delta) = r.
\]
Então
\begin{itemize}
\item[(i)] $\delta$ é minimax e
\item[(ii)] a sequência $\{\Lambda_n\}$ é menos favorável.
\end{itemize}

\subsection{Lema 1.13 - \textit{var}[\texorpdfstring{\(g(\theta)|X\)}{undefined}]} Se $\delta_{\Lambda}$ é o estimador de Bayes de $g(\theta)$ com respeito a $\Lambda$ e se $r_{\Lambda}$ é seu risco de Bayes, então
\[
r_{\Lambda} = E[(\delta_{\Lambda}(X) - g(\theta))^2]
\]
é seu risco de Bayes, então
\[
r_{\Lambda} = \int \text{var}[g(\theta)|X] dP(X).
\]
Em particular, se a variância posterior de $g(\theta)|X$ é independente de $X$, então
\[
r_{\Lambda} = \text{var}[g(\theta)|X].
\]

\subsection{Teorema 2.6 - Estimador inadmissível em \texorpdfstring{\(aX+b\)}{undefined}}
Seja $X$ uma variável aleatória com média $\theta$ e variância $\sigma^2$. Então, $aX + b$ é um estimador inadmissível de $\theta$ sob perda de erro quadrado sempre que
\begin{itemize}
\item[(i)] $a > 1$, ou
\item[(ii)] $a < 0$, ou
\item[(iii)] $a = 1$ e $b \neq 0$.
\end{itemize}
